{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "762abac3-d873-42c8-8ffb-cca1e8cdd336",
   "metadata": {},
   "source": [
    "# Inference 2 Consumer 1 as geting the data from producer 1 and extract features, then saved on HDFS, as a producer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48cc5360-a899-4d98-8c9a-3af64ab3f538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"./spark-3.5.3-bin-hadoop3\"\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.2.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "204b9098-f329-45ce-a7e5-24f35721411e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/root/music/spark-3.5.3-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-streaming-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-cab1483e-3bd4-4e14-8c1a-21f53f66e3de;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-streaming-kafka-0-10_2.12;3.2.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.0 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.1 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.1 in central\n",
      "\tfound org.apache.htrace#htrace-core4;4.1.0-incubating in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.2.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 461ms :: artifacts dl 14ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.1 from central in [default]\n",
      "\torg.apache.htrace#htrace-core4;4.1.0-incubating from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.2.0 from central in [default]\n",
      "\torg.apache.spark#spark-streaming-kafka-0-10_2.12;3.2.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   14  |   0   |   0   |   0   ||   14  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-cab1483e-3bd4-4e14-8c1a-21f53f66e3de\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 14 already retrieved (0kB/10ms)\n",
      "24/10/28 01:40:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/28 01:40:55 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/10/28 01:40:55 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "24/10/28 01:40:55 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "24/10/28 01:40:55 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "24/10/28 01:40:55 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "if 'spark_inf' in locals():\n",
    "    spark_inf.stop()\n",
    "\n",
    "# Initialize Spark session with Kafka support\n",
    "spark_inf = SparkSession.builder \\\n",
    "    .appName(\"RealTimeModelInference\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.7.1\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ccb5cc5-32d3-45d8-8cde-e716a504be40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df_kafka = spark_inf.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"audio_files\") \\\n",
    "    .load()\n",
    "\n",
    "# Convert binary Kafka value to string to get the file path\n",
    "audio_files_df = df_kafka.selectExpr(\"CAST(value AS BINARY) as content\")\n",
    "##\n",
    "#audio_files_df.writeStream.format(\"console\").start().awaitTermination()\n",
    "df_binary_with_name = df_kafka.selectExpr(\"CAST(key AS STRING) as file_name\", \"CAST(value AS BINARY) as content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27b4ebc2-9b20-4c0e-90aa-eccf229e4fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ipynb.fs.full.Main import extract_features_from_content\n",
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "# Feature extraction function\n",
    "def extract_features_from_content(content):\n",
    "\n",
    "    if len(content) % 4 == 0:\n",
    "        dtype = np.float32  # Assuming float32 corresponds to 4 bytes\n",
    "    elif len(content) % 2 == 0:\n",
    "        dtype = np.int16   # Assuming int16 corresponds to 2 bytes\n",
    "    else:\n",
    "        print(\"Content length is not compatible with int16 or float32.\")\n",
    "        print(f\"Content length: {len(content)}\")\n",
    "        #return [] \n",
    "    \n",
    "    audio_data = np.frombuffer(content, dtype=dtype)\n",
    "    \n",
    "    # Normalize the data for librosa processing\n",
    "    y = librosa.util.buf_to_float(audio_data, dtype=np.float32)\n",
    "    sr = 22050  # Assign a fixed sample rate\n",
    "\n",
    "    \n",
    "    # Extract features\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "    mfcc_mean = np.mean(mfcc, axis=1)\n",
    "    mfcc_std = np.std(mfcc, axis=1)\n",
    "    \n",
    "    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "    chroma_mean = np.mean(chroma, axis=1)\n",
    "    chroma_std = np.std(chroma, axis=1)\n",
    "    \n",
    "    spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "    spectral_centroid_mean = np.mean(spectral_centroid)\n",
    "    spectral_centroid_std = np.std(spectral_centroid)\n",
    "    \n",
    "    spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
    "    spectral_contrast_mean = np.mean(spectral_contrast, axis=1)\n",
    "    spectral_contrast_std = np.std(spectral_contrast, axis=1)\n",
    "    \n",
    "    zero_crossing_rate = librosa.feature.zero_crossing_rate(y)\n",
    "    zero_crossing_rate_mean = np.mean(zero_crossing_rate)\n",
    "    zero_crossing_rate_std = np.std(zero_crossing_rate)\n",
    "    \n",
    "    spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "    spectral_rolloff_mean = np.mean(spectral_rolloff)\n",
    "    spectral_rolloff_std = np.std(spectral_rolloff)\n",
    "    \n",
    "    # Combine all features into one array\n",
    "    features = np.hstack([\n",
    "        mfcc_mean, mfcc_std,\n",
    "        chroma_mean, chroma_std,\n",
    "        [spectral_centroid_mean, spectral_centroid_std],\n",
    "        spectral_contrast_mean, spectral_contrast_std,\n",
    "        [zero_crossing_rate_mean, zero_crossing_rate_std],\n",
    "        [spectral_rolloff_mean, spectral_rolloff_std]\n",
    "    ])\n",
    "    \n",
    "    return features.tolist()\n",
    "\n",
    "# Define UDF to apply the feature extraction function to each row\n",
    "@pandas_udf(ArrayType(FloatType()))\n",
    "def extract_audio_features(content):\n",
    "    return content.apply(extract_features_from_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3392d2f-40dc-4c43-8f4c-047220d23ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/28 01:41:05 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-03d4a562-0f66-405c-9969-7d4fae903540. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/10/28 01:41:05 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/10/28 01:41:06 WARN AdminClientConfig: The configuration 'key.deserializer' was supplied but isn't a known config.\n",
      "24/10/28 01:41:06 WARN AdminClientConfig: The configuration 'value.deserializer' was supplied but isn't a known config.\n",
      "24/10/28 01:41:06 WARN AdminClientConfig: The configuration 'enable.auto.commit' was supplied but isn't a known config.\n",
      "24/10/28 01:41:06 WARN AdminClientConfig: The configuration 'max.poll.records' was supplied but isn't a known config.\n",
      "24/10/28 01:41:06 WARN AdminClientConfig: The configuration 'auto.offset.reset' was supplied but isn't a known config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid features to write for this batch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/28 01:42:10 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/10/28 01:42:15 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote valid features to hdfs://localhost:9000/user/hadoop/Music/Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/28 01:42:19 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/10/28 01:46:29 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/10/28 01:46:32 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/10/28 01:46:36 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote valid features to hdfs://localhost:9000/user/hadoop/Music/Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/28 01:51:39 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/10/28 01:51:43 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/10/28 01:51:47 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote valid features to hdfs://localhost:9000/user/hadoop/Music/Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/28 01:53:55 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/10/28 01:53:58 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/10/28 01:54:02 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote valid features to hdfs://localhost:9000/user/hadoop/Music/Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/28 01:57:25 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/10/28 01:57:29 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/10/28 01:57:32 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote valid features to hdfs://localhost:9000/user/hadoop/Music/Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/28 01:57:33 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/10/28 01:57:34 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/10/28 01:57:36 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote valid features to hdfs://localhost:9000/user/hadoop/Music/Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/28 01:57:56 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/10/28 01:57:57 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/10/28 01:57:58 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote valid features to hdfs://localhost:9000/user/hadoop/Music/Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/28 01:57:59 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/10/28 01:58:00 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/10/28 01:58:01 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote valid features to hdfs://localhost:9000/user/hadoop/Music/Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/28 01:58:02 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/10/28 01:58:02 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/10/28 01:58:03 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/10/28 01:58:03 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/10/28 01:58:05 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/10/28 01:58:05 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote valid features to hdfs://localhost:9000/user/hadoop/Music/Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/28 01:58:06 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/10/28 01:58:06 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/10/28 01:58:07 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/10/28 01:58:07 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/10/28 01:58:09 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/10/28 01:58:09 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote valid features to hdfs://localhost:9000/user/hadoop/Music/Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/28 02:05:38 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/10/28 02:05:42 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/10/28 02:05:46 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote valid features to hdfs://localhost:9000/user/hadoop/Music/Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/28 02:06:14 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/10/28 02:06:15 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/10/28 02:06:17 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote valid features to hdfs://localhost:9000/user/hadoop/Music/Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/28 02:06:49 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/10/28 02:06:50 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/10/28 02:06:52 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote valid features to hdfs://localhost:9000/user/hadoop/Music/Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/28 02:07:30 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/10/28 02:07:31 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/10/28 02:07:32 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote valid features to hdfs://localhost:9000/user/hadoop/Music/Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/28 02:08:05 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/10/28 02:08:06 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/10/28 02:08:08 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote valid features to hdfs://localhost:9000/user/hadoop/Music/Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/28 02:09:46 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/10/28 02:09:49 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/10/28 02:09:53 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote valid features to hdfs://localhost:9000/user/hadoop/Music/Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/28 02:13:16 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/10/28 02:13:20 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/10/28 02:13:23 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote valid features to hdfs://localhost:9000/user/hadoop/Music/Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/28 02:13:31 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/10/28 02:13:32 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/10/28 02:13:34 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote valid features to hdfs://localhost:9000/user/hadoop/Music/Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/28 02:13:57 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/10/28 02:13:58 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/10/28 02:13:59 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote valid features to hdfs://localhost:9000/user/hadoop/Music/Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/28 02:14:07 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/10/28 02:14:08 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/10/28 02:14:10 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote valid features to hdfs://localhost:9000/user/hadoop/Music/Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/28 02:14:33 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/10/28 02:14:34 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n",
      "24/10/28 02:14:35 WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote valid features to hdfs://localhost:9000/user/hadoop/Music/Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "### validate the empty and process data , save as parquet on HDFS\n",
    "from pyspark.sql.functions import lit, size\n",
    "\n",
    "# Define the process_audio_batch function\n",
    "hdfs_path = \"hdfs://localhost:9000/user/hadoop/Music/Inference\"\n",
    "\n",
    "def process_audio_batch(df, epoch_id):\n",
    "\n",
    "    # Extract features using the UDF\n",
    "    features_df = df.withColumn(\"audio_features\", extract_audio_features(\"content\"))\n",
    "\n",
    "    # Filter to only keep non-empty audio_features before writing\n",
    "    valid_features_df = features_df.filter(features_df[\"audio_features\"].isNotNull() & (size(features_df[\"audio_features\"]) > 0))\n",
    "\n",
    "    if valid_features_df.count() > 0:\n",
    "        # Mark records as processed and write to HDFS\n",
    "        valid_features_df = valid_features_df.withColumn(\"processed\", lit(True))\n",
    "        valid_features_df.write.mode(\"append\").parquet(hdfs_path)\n",
    "        print(f\"Successfully wrote valid features to {hdfs_path}\")\n",
    "    else:\n",
    "        print(\"No valid features to write for this batch.\")\n",
    "\n",
    "    # Optionally, log and show any empty audio features for debugging\n",
    "    empty_features = features_df.filter(features_df[\"audio_features\"].isNull() | (size(features_df[\"audio_features\"]) == 0))\n",
    "    if empty_features.count() > 0:\n",
    "        print(\"Warning: Found empty audio_features in the following records:\")\n",
    "        empty_features.show(truncate=False)\n",
    "\n",
    "# Set up the query to process each batch and run inference\n",
    "query = audio_files_df.writeStream \\\n",
    "    .foreachBatch(process_audio_batch) \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b89e2f6c-bb81-4897-962f-cc569a6fc74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_inf.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb05156-a0c5-4b0c-9981-2bac2c5ecad5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
